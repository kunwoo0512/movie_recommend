#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
SentenceBERT ê¸°ë°˜ ì˜í™” ì¤„ê±°ë¦¬ ì„ë² ë”© ìƒì„±ê¸° (ìœˆë„ìš° ì¸ë±ì‹± ë²„ì „)
- í•œêµ­ì–´ ì¿¼ë¦¬ â†” ì˜ì–´(ë˜ëŠ” í˜¼í•©) ì¤„ê±°ë¦¬ ë§¤ì¹­
- ë¬¸ì¥ ë¶„í•  í›„ 2~3ë¬¸ì¥ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì„ë² ë”©
- L2 ì •ê·œí™” ë° ì œë¡œë²¡í„°/NaN ê°€ë“œ
- ê²°ê³¼: embeddings.npy + metadata.jsonl (FAISS í˜¸í™˜)
"""

"""python bert_embedding_generator_new.py --input movies_dataset.json --output_dir data --window_size 2 --stride 1 --batch_size 16
1. build_faiss_and_query.py --build ë¡œ FAISS ì¸ë±ìŠ¤ ìƒì„±
2. build_faiss_and_query.py --demo_query 'ê²€ìƒ‰ì–´' ë¡œ í…ŒìŠ¤íŠ¸

"""

import os
import re
import json
import time
import argparse
import warnings
from typing import List, Dict, Tuple

import numpy as np
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import torch

warnings.filterwarnings("ignore")


# -----------------------------
# ìœ í‹¸: ë¬¸ì¥ ë¶„í•  & ìœˆë„ìš° êµ¬ì„±
# -----------------------------

def split_sentences(text: str) -> List[str]:
    """ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´ ê°€ë²¼ìš´ ë¬¸ì¥ ë¶„í• """
    if not text:
        return []
    
    # ê³µë°± ì •ëˆ
    t = re.sub(r"\s+", " ", text.strip())
    
    # ë¬¸ì¥ ê²½ê³„ íœ´ë¦¬ìŠ¤í‹± (ê°„ë‹¨/ê²½ëŸ‰)
    sent_boundaries = re.compile(
        r"""
        (?<=[\.!\?])\s+       |   # ì˜ë¬¸ .!? ë’¤ ê³µë°±
        (?<=[ë‹¤ìš”ì£ ìŒì„ë‹ˆë‹ˆê¹Œê¹Œ]\.)\s+ |   # í•œê¸€ í”í•œ ì¢…ê²° + ë§ˆì¹¨í‘œ
        (?<=[ë‹¤ìš”ì£ ìŒì„ë‹ˆë‹ˆê¹Œê¹Œ])\s+(?=[A-Zê°€-í£0-9])   # í•œê¸€ ì¢…ê²° ë’¤ ê³µë°±
        """,
        re.VERBOSE
    )
    
    # ë„ˆë¬´ ê¸´ ì¤„ ëŒ€ë¹„: êµ¬ë‘ì  ì—†ìœ¼ë©´ ëŒ€ê°• 120~200ìë§ˆë‹¤ ëŠê¸°
    if not re.search(r"[\.!\?]", t) and len(t) > 200:
        chunks = [t[i:i+180] for i in range(0, len(t), 180)]
        return [c.strip() for c in chunks if c.strip()]
    
    parts = re.split(sent_boundaries, t)
    sents = [p.strip() for p in parts if p and p.strip()]
    return sents

def build_windows(sents: List[str], window_size: int = 2, stride: int = 1, max_chars: int = 600) -> List[Tuple[int, int, str]]:
    """
    2~3ë¬¸ì¥ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°.
    ë°˜í™˜: (start_sent_idx, end_sent_idx_exclusive, window_text)
    """
    windows = []
    n = len(sents)
    if n == 0:
        return windows
    
    for start in range(0, max(1, n - window_size + 1), stride):
        end = min(n, start + window_size)
        chunk = " ".join(sents[start:end]).strip()
        if len(chunk) > max_chars:
            chunk = chunk[:max_chars]
        if chunk:
            windows.append((start, end, chunk))
    
    # ë§Œì•½ ë¬¸ì¥ì´ 1ê°œë¿ì´ë©´ ìµœì†Œ 1ì°½ í™•ë³´
    if not windows and n > 0:
        chunk = sents[0][:max_chars]
        windows.append((0, 1, chunk))
    
    return windows


# -----------------------------
# ì„ë² ë”© ë˜í¼
# -----------------------------

class SBertEmbedder:
    def __init__(self, model_name: str, device: str = None, batch_size: int = 32):
        self.model_name = model_name
        device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"[ëª¨ë¸] {model_name} ë¡œë”© ì¤‘... (ì¥ì¹˜: {device})")
        self.model = SentenceTransformer(model_name, device=device)
        self.batch_size = batch_size
        print(f"[ì™„ë£Œ] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì°¨ì›: {self.get_dim()})")

    def encode_texts(self, texts: List[str]) -> Tuple[np.ndarray, List[int]]:
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ â†’ L2 ì •ê·œí™”ëœ ì„ë² ë”© (float32)
        - normalize_embeddings=True ë¡œ ë‚´ì =ì½”ì‚¬ì¸
        - ë°˜í™˜: (valid_embeddings, valid_indices)
        """
        if not texts:
            return np.zeros((0, self.get_dim()), dtype="float32"), []
        
        print(f"[ì„ë² ë”©] {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘...")
        vecs = self.model.encode(
            texts,
            convert_to_numpy=True,
            normalize_embeddings=True,   # â˜… ì¤‘ìš”: ì •ê·œí™”
            batch_size=self.batch_size,
            show_progress_bar=True
        ).astype("float32")

        # NaN/Inf/ì œë¡œ ê°€ë“œ
        valid_mask = (
            np.isfinite(vecs).all(axis=1) &
            (np.linalg.norm(vecs, axis=1) >= 1e-6)
        )
        
        if (~valid_mask).any():
            invalid_count = (~valid_mask).sum()
            print(f"[ê²½ê³ ] ë¹„ì •ìƒ ì„ë² ë”© {invalid_count}ê°œ ì œê±° (ë‚¨ì€ {valid_mask.sum()}ê°œ)")
            vecs = vecs[valid_mask]
            valid_indices = [i for i, v in enumerate(valid_mask) if v]
        else:
            valid_indices = list(range(len(texts)))
            
        return vecs, valid_indices

    def get_dim(self) -> int:
        try:
            return int(self.model.get_sentence_embedding_dimension())
        except Exception:
            return 384


# -----------------------------
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# -----------------------------

def process_dataset(
    input_json: str,
    output_dir: str,
    model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    window_size: int = 2,
    stride: int = 1,
    batch_size: int = 32,
    use_windows: bool = True
):
    os.makedirs(output_dir, exist_ok=True)
    print(f"\n[ì„¤ì •] model={model_name}")
    print(f"[ì„¤ì •] window_size={window_size}, stride={stride}, batch_size={batch_size}")
    print(f"[ì…ë ¥] {input_json}")
    print(f"[ì¶œë ¥] {output_dir}")

    # 1) ë°ì´í„° ë¡œë“œ
    with open(input_json, "r", encoding="utf-8") as f:
        movies = json.load(f)
    assert isinstance(movies, list), "movies_dataset.jsonì€ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤."
    print(f"[ë¡œë“œ] {len(movies)}í¸ ì˜í™” ë°ì´í„° ë¡œë“œ")

    # 2) ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”
    embedder = SBertEmbedder(model_name=model_name, batch_size=batch_size)
    
    if use_windows:
        return _process_with_windows(movies, embedder, output_dir, window_size, stride)
    else:
        return _process_full_plots(movies, embedder, output_dir)

def _process_with_windows(movies, embedder, output_dir, window_size, stride):
    """ìœˆë„ìš° ê¸°ë°˜ ì²˜ë¦¬"""
    print(f"[ëª¨ë“œ] ìœˆë„ìš° ê¸°ë°˜ ì²˜ë¦¬ (window_size={window_size}, stride={stride})")
    
    # ì¤„ê±°ë¦¬ â†’ ë¬¸ì¥ â†’ ìœˆë„ìš°
    all_texts = []
    all_metadata = []
    
    for mi, movie in enumerate(tqdm(movies, desc="ìœˆë„ìš° ìƒì„±")):
        title = movie.get("title", f"Movie_{mi}")
        plot = (movie.get("plot") or "").strip()
        if not plot:
            plot = title
            
        sents = split_sentences(plot)
        windows = build_windows(sents, window_size=window_size, stride=stride)
        
        for wi, (start, end, text) in enumerate(windows):
            all_texts.append(text)
            all_metadata.append({
                "movie_index": mi,
                "title": title,
                "window_index": wi,
                "start_sent": start,
                "end_sent": end,
                "text": text,
                "year": movie.get("year", ""),
                "director": movie.get("director", ""),
                "genres": movie.get("genres", {}),
                "movie_id": movie.get("movie_id", f"movie_{mi}")
            })
    
    print(f"[í†µê³„] ì´ {len(all_texts)}ê°œ ìœˆë„ìš° ìƒì„±")
    
    # ì„ë² ë”© ìƒì„±
    embeddings, valid_indices = embedder.encode_texts(all_texts)
    
    # ìœ íš¨í•œ ë©”íƒ€ë°ì´í„°ë§Œ ìœ ì§€
    valid_metadata = [all_metadata[i] for i in valid_indices]
    
    # ì €ì¥ (FAISS í˜¸í™˜ í˜•ì‹)
    np.save(os.path.join(output_dir, "embeddings.npy"), embeddings)
    
    with open(os.path.join(output_dir, "metadata.jsonl"), "w", encoding="utf-8") as f:
        for meta in valid_metadata:
            f.write(json.dumps(meta, ensure_ascii=False) + "\n")
    
    print(f"[ì €ì¥] embeddings.npy: {embeddings.shape}")
    print(f"[ì €ì¥] metadata.jsonl: {len(valid_metadata)}ê°œ í•­ëª©")

def _process_full_plots(movies, embedder, output_dir):
    """ì „ì²´ ì¤„ê±°ë¦¬ ê¸°ë°˜ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)"""
    print(f"[ëª¨ë“œ] ì „ì²´ ì¤„ê±°ë¦¬ ê¸°ë°˜ ì²˜ë¦¬")
    
    all_texts = []
    all_metadata = []
    
    for mi, movie in enumerate(movies):
        title = movie.get("title", f"Movie_{mi}")
        plot = (movie.get("plot") or "").strip()
        if not plot:
            plot = title
            
        all_texts.append(plot)
        all_metadata.append({
            "movie_index": mi,
            "title": title,
            "plot": plot,
            "year": movie.get("year", ""),
            "director": movie.get("director", ""),
            "genres": movie.get("genres", {}),
            "movie_id": movie.get("movie_id", f"movie_{mi}")
        })
    
    # ì„ë² ë”© ìƒì„±
    embeddings, valid_indices = embedder.encode_texts(all_texts)
    
    # ìœ íš¨í•œ ë©”íƒ€ë°ì´í„°ë§Œ ìœ ì§€
    valid_metadata = [all_metadata[i] for i in valid_indices]
    
    # ì €ì¥ (FAISS í˜¸í™˜ í˜•ì‹)
    np.save(os.path.join(output_dir, "embeddings.npy"), embeddings)
    
    with open(os.path.join(output_dir, "metadata.jsonl"), "w", encoding="utf-8") as f:
        for meta in valid_metadata:
            f.write(json.dumps(meta, ensure_ascii=False) + "\n")
    
    print(f"[ì €ì¥] embeddings.npy: {embeddings.shape}")
    print(f"[ì €ì¥] metadata.jsonl: {len(valid_metadata)}ê°œ í•­ëª©")


# -----------------------------
# CLI
# -----------------------------

def main():
    parser = argparse.ArgumentParser(description="SentenceBERT ì˜í™” ì¤„ê±°ë¦¬ ì„ë² ë”© ìƒì„±ê¸°")
    parser.add_argument("--input", type=str, default="movies_dataset.json", help="ì…ë ¥ JSON ê²½ë¡œ")
    parser.add_argument("--output_dir", type=str, default="data", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--model", type=str, default="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", help="SentenceBERT ëª¨ë¸ëª…")
    parser.add_argument("--window_size", type=int, default=2, help="ìœˆë„ìš° ë¬¸ì¥ ìˆ˜ (2~3 ê¶Œì¥)")
    parser.add_argument("--stride", type=int, default=1, help="ìŠ¬ë¼ì´ë”© ìŠ¤íŠ¸ë¼ì´ë“œ")
    parser.add_argument("--batch_size", type=int, default=32, help="ì„ë² ë”© ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--no_windows", action="store_true", help="ìœˆë„ìš° ëª¨ë“œ ë¹„í™œì„±í™” (ì „ì²´ ì¤„ê±°ë¦¬ ì‚¬ìš©)")
    
    args = parser.parse_args()
    
    print("ğŸ¬ SentenceBERT ê¸°ë°˜ ì˜í™” ì„ë² ë”© ìƒì„±ê¸°")
    print("=" * 60)
    
    try:
        process_dataset(
            input_json=args.input,
            output_dir=args.output_dir,
            model_name=args.model,
            window_size=args.window_size,
            stride=args.stride,
            batch_size=args.batch_size,
            use_windows=not args.no_windows
        )
        
        print("\nâœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("1. build_faiss_and_query.py --build ë¡œ FAISS ì¸ë±ìŠ¤ ìƒì„±")
        print("2. build_faiss_and_query.py --demo_query 'ê²€ìƒ‰ì–´' ë¡œ í…ŒìŠ¤íŠ¸")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("\ní•´ê²° ë°©ë²•:")
        print("1. sentence-transformers ì„¤ì¹˜: pip install sentence-transformers")
        print("2. ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ --batch_size ì¤„ì´ê¸°")
        print("3. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ CPU ì‚¬ìš©")

if __name__ == "__main__":
    main()
