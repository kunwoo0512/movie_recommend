#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse, json, re
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import time

import numpy as np
import faiss

def load_metadata(jsonl_path: Path) -> List[Dict[str, Any]]:
    metas = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            metas.append(json.loads(line))
    return metas

def ensure_unit_norm(x: np.ndarray) -> np.ndarray:
    # ì½”ì‚¬ì¸=ë‚´ì ì„ ìœ„í•´ L2 ì •ê·œí™”
    x = x.astype('float32', copy=False)
    faiss.normalize_L2(x)
    return x

def build_faiss_index(embeddings: np.ndarray, out_index: Path, out_idmap: Path):
    dim = embeddings.shape[1]
    # ì•ˆì „ë§: í˜¹ì‹œ ì •ê·œí™” ì•ˆ ëœ ì„ë² ë”©ì´ ë“¤ì–´ì˜¤ë©´ ì—¬ê¸°ì„œ ì •ê·œí™”
    if not np.allclose((embeddings**2).sum(axis=1).mean(), 1.0, atol=1e-2):
        embeddings = ensure_unit_norm(embeddings)

    index = faiss.IndexFlatIP(dim)  # ì½”ì‚¬ì¸=IP (ì •ê·œí™” ì „ì œ)
    index.add(embeddings)
    faiss.write_index(index, str(out_index))
    id_map = np.arange(embeddings.shape[0], dtype=np.int64)
    np.save(out_idmap, id_map)
    return index

def query_index(index: faiss.Index, q_vecs: np.ndarray, top_k: int = 5):
    return index.search(q_vecs, top_k)

def aggregate_by_movie(sims: np.ndarray, ids: np.ndarray, metas: List[Dict[str, Any]], 
                      top_k: int = 5) -> Tuple[np.ndarray, np.ndarray, List[Dict[str, Any]]]:
    """ìœˆë„ìš° ê²°ê³¼ë¥¼ ì˜í™”ë³„ë¡œ ì§‘ê³„"""
    # ì˜í™”ë³„ ì ìˆ˜ ì§‘ê³„ (ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©)
    movie_scores = {}
    
    for qi in range(sims.shape[0]):  # ì¿¼ë¦¬ë³„ ì²˜ë¦¬
        for sid, score in zip(ids[qi], sims[qi]):
            try:
                meta = metas[int(sid)]
                movie_index = meta.get('movie_index')
                title = meta.get('title', f'Unknown_{movie_index}')
                
                if movie_index not in movie_scores:
                    movie_scores[movie_index] = {
                        'scores': [],
                        'meta': {
                            'title': title,
                            'year': meta.get('year', ''),
                            'director': meta.get('director', ''),
                            'movie_index': movie_index
                        }
                    }
                movie_scores[movie_index]['scores'].append(float(score))
            except (IndexError, KeyError) as e:
                print(f"[ê²½ê³ ] ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
    
    # ì˜í™”ë³„ ìµœì¢… ì ìˆ˜ ê³„ì‚° (ìƒìœ„ ì ìˆ˜ë“¤ì˜ ê°€ì¤‘ í‰ê· )
    final_movies = []
    for movie_index, data in movie_scores.items():
        scores = sorted(data['scores'], reverse=True)
        # ìƒìœ„ 3ê°œ ìœˆë„ìš°ì˜ ê°€ì¤‘ í‰ê·  (1.0, 0.7, 0.5)
        weights = [1.0, 0.7, 0.5]
        final_score = sum(s * w for s, w in zip(scores[:3], weights[:len(scores)])) / sum(weights[:len(scores)])
        
        final_movies.append({
            'score': final_score,
            'meta': data['meta']
        })
    
    # ì ìˆ˜ìˆœ ì •ë ¬ í›„ top_k ì„ íƒ
    final_movies.sort(key=lambda x: x['score'], reverse=True)
    final_movies = final_movies[:top_k]
    
    # numpy ë°°ì—´ í˜•íƒœë¡œ ë³€í™˜
    final_sims = np.array([[movie['score'] for movie in final_movies]], dtype='float32')
    final_ids = np.array([[i for i in range(len(final_movies))]], dtype='int64')
    final_metas = [movie['meta'] for movie in final_movies]
    
    return final_sims, final_ids, final_metas
    """ìœˆë„ìš° ì ìˆ˜ë¥¼ ì˜í™”ë³„ë¡œ ì§‘ê³„"""
    from collections import defaultdict
    
    # ì˜í™”ë³„ ì ìˆ˜ ìˆ˜ì§‘
    movie_scores = defaultdict(list)
    movie_info = {}
    
    for sid, sim in zip(ids[0], sims[0]):
        meta = metas[int(sid)]
        movie_idx = meta.get('movie_index', meta.get('title'))  # movie_index ë˜ëŠ” titleë¡œ ê·¸ë£¹í•‘
        
        movie_scores[movie_idx].append(float(sim))
        
        # ì˜í™” ì •ë³´ ì €ì¥ (ì²« ë²ˆì§¸ ìœˆë„ìš° ì •ë³´ ì‚¬ìš©)
        if movie_idx not in movie_info:
            movie_info[movie_idx] = {
                'title': meta.get('title', 'Unknown'),
                'year': meta.get('year', ''),
                'director': meta.get('director', ''),
                'movie_index': movie_idx
            }
    
    # ì˜í™”ë³„ ìµœì¢… ì ìˆ˜ ê³„ì‚° (ìƒìœ„ 3ê°œ ìœˆë„ìš° í‰ê· )
    movie_final_scores = []
    for movie_idx, scores in movie_scores.items():
        # ìƒìœ„ 3ê°œ ì ìˆ˜ì˜ ê°€ì¤‘ í‰ê·  (ì²« ë²ˆì§¸ê°€ ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜)
        scores = sorted(scores, reverse=True)
        if len(scores) >= 3:
            final_score = (scores[0] * 0.5 + scores[1] * 0.3 + scores[2] * 0.2)
        elif len(scores) == 2:
            final_score = (scores[0] * 0.7 + scores[1] * 0.3)
        else:
            final_score = scores[0]
        
        movie_final_scores.append((final_score, movie_idx))
    
    # ì ìˆ˜ìˆœ ì •ë ¬
    movie_final_scores.sort(reverse=True)
    
    # ê²°ê³¼ ì¬êµ¬ì„±
    final_sims = np.array([[score for score, _ in movie_final_scores]], dtype='float32')
    final_ids = np.array([[movie_idx for _, movie_idx in movie_final_scores]], dtype='int64')
    final_metas = [movie_info[movie_idx] for _, movie_idx in movie_final_scores]
    
    return final_sims, final_ids, final_metas


# -----------------------------
# LLM í•„í„°ë§ í´ë˜ìŠ¤  
# -----------------------------

class LLMMovieFilter:
    def __init__(self, api_key: str):
        try:
            import openai
            self.client = openai.OpenAI(api_key=api_key)
            self.available = True
        except ImportError:
            print("[ê²½ê³ ] openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. LLM í•„í„°ë§ ë¹„í™œì„±í™”")
            self.available = False
        except Exception as e:
            print(f"[ê²½ê³ ] OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.available = False

    def filter_search_results(self, query: str, movies: List[Dict], threshold: int = 7) -> List[Dict]:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLMìœ¼ë¡œ í•„í„°ë§"""
        if not self.available:
            print("[ìŠ¤í‚µ] LLM í•„í„°ë§ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ")
            return movies

        print(f"[LLM í•„í„°ë§] {len(movies)}ê°œ ì˜í™” ë¶„ì„ ì¤‘...")
        filtered_movies = []
        
        for i, movie in enumerate(movies, 1):
            try:
                print(f"  [{i}/{len(movies)}] {movie.get('title', 'Unknown')} ë¶„ì„ ì¤‘...")
                
                judgment = self._analyze_movie_relevance(query, movie)
                
                if judgment['pass'] and judgment['score'] >= threshold:
                    filtered_movies.append({
                        **movie,
                        'llm_score': judgment['score'],
                        'llm_reason': judgment['reason']
                    })
                    print(f"    âœ… PASS (ì ìˆ˜: {judgment['score']}/10)")
                else:
                    print(f"    âŒ FAIL (ì ìˆ˜: {judgment['score']}/10) - {judgment['reason']}")
                
                time.sleep(0.5)  # API ìš”ì²­ ê°„ê²©
                
            except Exception as e:
                print(f"    âš ï¸ ì˜¤ë¥˜: {e}")
                filtered_movies.append(movie)
        
        print(f"[LLM í•„í„°ë§ ì™„ë£Œ] {len(movies)} â†’ {len(filtered_movies)}ê°œ")
        return filtered_movies

    def _analyze_movie_relevance(self, query: str, movie: Dict) -> Dict:
        """ê°œë³„ ì˜í™”ì˜ ê²€ìƒ‰ì–´ ì í•©ì„± ë¶„ì„"""
        prompt = self._create_filter_prompt(query, movie)
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì •í™•ë„ë¥¼ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì°¾ëŠ” ì˜í™”ì˜ íŠ¹ì„±ì„ ì •í™•íˆ íŒŒì•…í•˜ê³ , ì¤„ê±°ë¦¬ì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=300,  # ë” ê¸´ ì‘ë‹µì„ ìœ„í•´ ì¦ê°€
                temperature=0.3
            )
            
            content = response.choices[0].message.content
            return self._parse_llm_response(content)
            
        except Exception as e:
            print(f"[API ì˜¤ë¥˜] {e}")
            return {"pass": True, "score": 5, "reason": "API ì˜¤ë¥˜ë¡œ ê¸°ë³¸ í†µê³¼"}

    def _create_filter_prompt(self, query: str, movie: Dict) -> str:
        """í•„í„°ë§ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        title = movie.get('title', 'Unknown')
        year = movie.get('year', 'Unknown')
        plot = movie.get('plot', '')[:800]  # ë” ê¸´ ì¤„ê±°ë¦¬ í¬í•¨
        
        return f"""
ê²€ìƒ‰ì–´: "{query}"

ì˜í™” ì •ë³´:
ì œëª©: {title} ({year})
ì¤„ê±°ë¦¬: {plot}

ì´ ì˜í™”ê°€ ê²€ìƒ‰ì–´ì˜ ì˜ë„ì— ë¶€í•©í•˜ëŠ”ì§€ ìƒì„¸íˆ í‰ê°€í•´ì£¼ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
1. ê²€ìƒ‰ì–´ì˜ í•µì‹¬ í‚¤ì›Œë“œ/í…Œë§ˆê°€ ì¤„ê±°ë¦¬ì— êµ¬ì²´ì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ”ê°€?
2. ê²€ìƒ‰ì–´ì˜ ì˜ë„(ì¥ë¥´, ê°ì •, ìƒí™©, ìºë¦­í„° ë“±)ì™€ ì˜í™” ë‚´ìš©ì´ ì¼ì¹˜í•˜ëŠ”ê°€?
3. ë‹¨ìˆœí•œ í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì•„ë‹Œ ì‹¤ì œ ìŠ¤í† ë¦¬/í…Œë§ˆ ì—°ê´€ì„±ì´ ìˆëŠ”ê°€?

ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:
íŒì •: [PASS ë˜ëŠ” FAIL]
ì ìˆ˜: [1-10]
ì´ìœ : [ì¤„ê±°ë¦¬ì˜ êµ¬ì²´ì ì¸ ë¶€ë¶„ì„ ì¸ìš©í•˜ë©° ì™œ ê²€ìƒ‰ì–´ì™€ ì—°ê´€ë˜ëŠ”ì§€ 2-3ë¬¸ì¥ìœ¼ë¡œ ìƒì„¸ ì„¤ëª…]

ì˜ˆì‹œ:
íŒì •: PASS
ì ìˆ˜: 8
ì´ìœ : ì¤„ê±°ë¦¬ì—ì„œ "ì¢€ë¹„ ë°”ì´ëŸ¬ìŠ¤ê°€ ì „ ì„¸ê³„ë¡œ í¼ì ¸ë‚˜ê°€ë©° ì¸ë¥˜ê°€ ê°ì—¼ëœ ì¢€ë¹„ë“¤ê³¼ ì‹¸ìš°ëŠ”" ë‚´ìš©ì´ ë‚˜íƒ€ë‚˜ë¯€ë¡œ 'ì¢€ë¹„' ê²€ìƒ‰ì–´ì™€ ì§ì ‘ì ìœ¼ë¡œ ì¼ì¹˜í•©ë‹ˆë‹¤. íŠ¹íˆ "ì¢€ë¹„ ë¬´ë¦¬ë“¤ì´ ë„ì‹œë¥¼ ìŠµê²©í•˜ê³  ìƒì¡´ìë“¤ì´ ì•ˆì „í•œ ê³³ì„ ì°¾ì•„ ë„ë§ì¹˜ëŠ”" ì¥ë©´ë“¤ì´ ì „í˜•ì ì¸ ì¢€ë¹„ ì¥ë¥´ì˜ í•µì‹¬ ìš”ì†Œë¥¼ í¬í•¨í•˜ê³  ìˆì–´ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì˜í™”ì™€ ì •í™•íˆ ë¶€í•©í•©ë‹ˆë‹¤.
"""

    def _parse_llm_response(self, response: str) -> Dict:
        """LLM ì‘ë‹µ íŒŒì‹±"""
        try:
            pass_match = re.search(r'íŒì •:\s*(PASS|FAIL)', response, re.IGNORECASE)
            score_match = re.search(r'ì ìˆ˜:\s*(\d+)', response)
            reason_match = re.search(r'ì´ìœ :\s*(.+)', response, re.DOTALL)  # ì—¬ëŸ¬ ì¤„ ì´ìœ  ì§€ì›
            
            pass_result = pass_match.group(1).upper() == 'PASS' if pass_match else True
            score = int(score_match.group(1)) if score_match else 5
            reason = reason_match.group(1).strip() if reason_match else "ë¶„ì„ ì™„ë£Œ"
            
            # ì´ìœ ê°€ ë„ˆë¬´ ê¸¸ë©´ ì²« ë²ˆì§¸ ë¬¸ì¥ë“¤ë§Œ ì‚¬ìš© (ìµœëŒ€ 200ì)
            if len(reason) > 200:
                sentences = reason.split('. ')
                truncated = sentences[0]
                for sent in sentences[1:]:
                    if len(truncated + '. ' + sent) <= 200:
                        truncated += '. ' + sent
                    else:
                        break
                reason = truncated + '.' if not truncated.endswith('.') else truncated
            
            return {
                "pass": pass_result,
                "score": max(1, min(10, score)),
                "reason": reason
            }
            
        except Exception as e:
            return {"pass": True, "score": 5, "reason": "íŒŒì‹± ì˜¤ë¥˜"}


# -----------------------------
# ì›¹ ì—°ë™ì„ ìœ„í•œ ê²°ê³¼ í¬ë§·í„°
# -----------------------------

def format_results_for_web(movies: List[Dict], query: str, llm_filtered: bool = False) -> Dict:
    """ì›¹í˜ì´ì§€ í‘œì‹œìš© ê²°ê³¼ í¬ë§·"""
    formatted_results = {
        "query": query,
        "total_results": len(movies),
        "llm_filtered": llm_filtered,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "movies": []
    }
    
    for i, movie in enumerate(movies, 1):
        formatted_movie = {
            "rank": i,
            "title": movie.get('title', 'Unknown'),
            "year": movie.get('year', 'Unknown'),
            "director": movie.get('director', 'Unknown'),
            "plot": movie.get('plot', ''),
            "score": float(movie.get('score', 0.0)),
            "poster_url": f"/static/posters/{movie.get('title', 'default').replace(' ', '_')}.jpg",
            "llm_analysis": {
                "score": movie.get('llm_score'),
                "reason": movie.get('llm_reason')
            } if llm_filtered and 'llm_score' in movie else None
        }
        formatted_results["movies"].append(formatted_movie)
    
    return formatted_results


def save_results_for_web(results: Dict, output_file: str = "web_results.json"):
    """ì›¹ ì—°ë™ìš© ê²°ê³¼ ì €ì¥"""
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"[ì›¹ ì—°ë™] ê²°ê³¼ ì €ì¥: {output_file}")
    except Exception as e:
        print(f"[ì˜¤ë¥˜] ì›¹ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")


def print_results(sims, ids, metas: List[Dict[str, Any]]):
    for qi in range(ids.shape[0]):
        print(f"\n[Query {qi}] Top-{ids.shape[1]} results")
        for rank, (sid, sim) in enumerate(zip(ids[qi], sims[qi]), start=1):
            # sidëŠ” ì´ì œ ì§‘ê³„ëœ ì˜í™”ì˜ ì¸ë±ìŠ¤
            m = metas[int(sid)]
            print(f"  {rank:>2}. score={sim:.4f} | {m.get('title')} ({m.get('year')}) | dir={m.get('director')}")

def interactive_search():
    """ëŒ€í™”í˜• ê²€ìƒ‰ ëª¨ë“œ"""
    import os
    from dotenv import load_dotenv
    from weighted_search_utils import get_weighted_helper
    
    # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()
    openai_key = os.getenv('OPENAI_API_KEY')
    
    print("=" * 60)
    print("ğŸ¬ ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ")
    print("=" * 60)
    print("1. ê¸°ì¡´ ê²€ìƒ‰ (ì²­í‚¹ ê¸°ë°˜ + LLM í•„í„°ë§)")
    print("2. ê°€ì¤‘ì¹˜ ì¡°ì ˆ ê²€ìƒ‰ (ë¶„ë¦¬ ì„ë² ë”©)")
    print("=" * 60)
    
    search_mode = input("ê²€ìƒ‰ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš” (1 ë˜ëŠ” 2): ").strip()
    
    if search_mode == "2":
        weighted_interactive_search()
        return
    
    print("=" * 60)
    print("ğŸ¬ ê¸°ì¡´ ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ (SentenceBERT + LLM í•„í„°ë§)")
    print("=" * 60)
    print("â€¢ 20ê°œ í›„ë³´ ê²€ìƒ‰ â†’ LLM ê²€ì¦ â†’ ìµœì¢… 5ê°œ ì¶”ì²œ")
    print("â€¢ 'quit' ë˜ëŠ” 'exit' ì…ë ¥ì‹œ ì¢…ë£Œ")
    print("=" * 60)
    
    # í•„ìš”í•œ íŒŒì¼ë“¤ ì²´í¬
    data_dir = Path('data')
    required_files = {
        'embeddings.npy': data_dir / 'embeddings.npy',
        'metadata.jsonl': data_dir / 'metadata.jsonl', 
        'index.faiss': data_dir / 'index.faiss'
    }
    
    missing_files = [name for name, path in required_files.items() if not path.exists()]
    if missing_files:
        print(f"âŒ í•„ìš”í•œ íŒŒì¼ë“¤ì´ ì—†ìŠµë‹ˆë‹¤: {missing_files}")
        print("ë¨¼ì € ë‹¤ìŒ ëª…ë ¹ì–´ë“¤ì„ ì‹¤í–‰í•˜ì„¸ìš”:")
        print("1. python create_chunk_embeddings.py")
        print("2. python build_faiss_and_query.py --build")
        return
    
    # SentenceTransformer ëª¨ë¸ ë¡œë“œ
    print("ğŸ“ ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© ì¤‘...")
    try:
        from sentence_transformers import SentenceTransformer
        import torch
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device=device)
        
        # ë°ì´í„° ë¡œë“œ
        embeddings = np.load(required_files['embeddings.npy'])
        metas = load_metadata(required_files['metadata.jsonl'])
        index = faiss.read_index(str(required_files['index.faiss']))
        
        print(f"âœ… ë¡œë”© ì™„ë£Œ: {embeddings.shape[0]}ê°œ ìœˆë„ìš°, {len(set(m.get('movie_index') for m in metas))}ê°œ ì˜í™”")
        
        # LLM í•„í„° ì´ˆê¸°í™”
        llm_filter = None
        if openai_key:
            llm_filter = LLMMovieFilter(openai_key)
            if llm_filter.available:
                print("âœ… LLM í•„í„°ë§ í™œì„±í™”")
            else:
                print("âš ï¸ LLM í•„í„°ë§ ë¹„í™œì„±í™” (API í‚¤ ë¬¸ì œ)")
        else:
            print("âš ï¸ LLM í•„í„°ë§ ë¹„í™œì„±í™” (OPENAI_API_KEY ì—†ìŒ)")
            
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # ëŒ€í™”í˜• ê²€ìƒ‰ ë£¨í”„
    while True:
        try:
            print("\n" + "â”€" * 40)
            query = input("ğŸ” ì–´ë–¤ ì˜í™”ë¥¼ ì°¾ê³  ê³„ì‹ ê°€ìš”? > ").strip()
            
            if query.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                print("ğŸ‘‹ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
                
            if not query:
                print("âŒ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            print(f"\nğŸ” '{query}' ê²€ìƒ‰ ì¤‘...")
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            with torch.inference_mode():
                q = model.encode([query], convert_to_numpy=True, normalize_embeddings=False).astype('float32')
            q = ensure_unit_norm(q)
            
            # FAISS ê²€ìƒ‰ (20ê°œ)
            print("ğŸ“Š ë²¡í„° ê²€ìƒ‰ ì¤‘...")
            sims, ids = query_index(index, q, top_k=60)  # ë„‰ë„‰íˆ ê²€ìƒ‰
            
            # ì˜í™”ë³„ ì§‘ê³„ (20ê°œ)
            print("ğŸ¯ ì˜í™”ë³„ ì ìˆ˜ ì§‘ê³„ ì¤‘...")
            final_sims, final_ids, final_metas = aggregate_by_movie(sims, ids, metas, top_k=20)
            
            # ë©”íƒ€ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜
            candidate_movies = []
            for i, meta in enumerate(final_metas):
                candidate_movies.append({
                    **meta,
                    'score': float(final_sims[0][i])
                })
            
            # LLM í•„í„°ë§ (5ê°œ)
            final_movies = candidate_movies
            if llm_filter and llm_filter.available:
                print("ğŸ¤– LLM ê²€ì¦ ì¤‘...")
                final_movies = llm_filter.filter_search_results(
                    query=query,
                    movies=candidate_movies[:20],  # ìƒìœ„ 20ê°œ LLM ê²€ì¦
                    threshold=6  # ì„ê³„ê°’ì„ 6ìœ¼ë¡œ ë‚®ì¶¤
                )
                
                if not final_movies:
                    print("âš ï¸ LLM ê²€ì¦ì„ í†µê³¼í•œ ì˜í™”ê°€ ì—†ì–´ì„œ ì›ë³¸ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
                    final_movies = candidate_movies  # ëª¨ë“  í›„ë³´ í‘œì‹œ
            else:
                final_movies = candidate_movies  # LLM ì—†ìœ¼ë©´ ëª¨ë“  í›„ë³´
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ¬ '{query}' ê²€ìƒ‰ ê²°ê³¼")
            print("=" * 60)
            
            if not final_movies:
                print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            for i, movie in enumerate(final_movies, 1):  # 5ê°œ ì œí•œ í•´ì œ
                title = movie.get('title', 'Unknown')
                year = movie.get('year', 'Unknown') 
                director = movie.get('director', 'Unknown')
                score = movie.get('score', 0)
                
                print(f"\n{i}. {title} ({year})")
                print(f"   ê°ë…: {director}")
                print(f"   ìœ ì‚¬ë„: {score:.3f}")
                
                # LLM ë¶„ì„ ê²°ê³¼
                if 'llm_score' in movie:
                    llm_score = movie.get('llm_score', 0)
                    llm_reason = movie.get('llm_reason', '')
                    print(f"   LLM ì ìˆ˜: {llm_score}/10")
                    print(f"   ì¶”ì²œ ì´ìœ : {llm_reason}")
            
            # ì›¹ ì—°ë™ìš© ê²°ê³¼ ì €ì¥
            web_results = format_results_for_web(
                movies=final_movies,
                query=query,
                llm_filtered=(llm_filter and llm_filter.available)
            )
            save_results_for_web(web_results)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--data_dir', default='data')
    ap.add_argument('--build', action='store_true', help='FAISS ì¸ë±ìŠ¤ ìƒì„±')
    ap.add_argument('--demo_query', type=str, default='')
    ap.add_argument('--top_k', type=int, default=5)
    ap.add_argument('--model', default='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
    # ğŸ†• LLM í•„í„°ë§ ì˜µì…˜ë“¤
    ap.add_argument('--llm_filter', action='store_true', help='LLM ê¸°ë°˜ ê²°ê³¼ í•„í„°ë§ í™œì„±í™”')
    ap.add_argument('--openai_key', type=str, help='OpenAI API í‚¤')
    ap.add_argument('--llm_threshold', type=int, default=7, help='LLM í•„í„°ë§ ì„ê³„ê°’ (1-10)')
    ap.add_argument('--save_web', action='store_true', help='ì›¹ ì—°ë™ìš© ê²°ê³¼ ì €ì¥')
    args = ap.parse_args()
    
    # ğŸ†• ì¸ìˆ˜ê°€ ì—†ìœ¼ë©´ ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰
    import sys
    if len(sys.argv) == 1:
        interactive_search()
        return

    data_dir = Path(args.data_dir)
    emb_path = data_dir / 'embeddings.npy'
    meta_path = data_dir / 'metadata.jsonl'
    index_path = data_dir / 'index.faiss'
    idmap_path = data_dir / 'id_map.npy'

    embeddings = np.load(emb_path)          # (N, D)
    metas = load_metadata(meta_path)

    if args.build:
        print('[1/3] Building FAISS index')
        build_faiss_index(embeddings, index_path, idmap_path)
        print(f'âœ… Saved index to {index_path} and id_map to {idmap_path}')

    if args.demo_query:
        print('[2/3] Encoding demo query')
        from sentence_transformers import SentenceTransformer
        import torch
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = SentenceTransformer(args.model, device=device)

        with torch.inference_mode():
            q = model.encode([args.demo_query], convert_to_numpy=True, normalize_embeddings=False).astype('float32')
        q = ensure_unit_norm(q)  # ì¿¼ë¦¬ë„ ì •ê·œí™”

        # ì°¨ì› ê²€ì¦(ëª¨ë¸ í†µì¼ í™•ì¸)
        if q.shape[1] != embeddings.shape[1]:
            raise ValueError(f"Dim mismatch: query({q.shape[1]}) vs corpus({embeddings.shape[1]}). "
                             f"--modelì„ ì„ë² ë”© ìƒì„± ì‹œ ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼í•˜ê²Œ ë§ì¶”ì„¸ìš”.")

        print('[3/3] Searching')
        index = faiss.read_index(str(index_path))
        sims, ids = query_index(index, q, top_k=args.top_k * 3)  # ë” ë§ì´ ê²€ìƒ‰í•´ì„œ ì§‘ê³„

        # ìœˆë„ìš° ì ìˆ˜ë¥¼ ì˜í™”ë³„ë¡œ ì§‘ê³„
        print('[4/4] Aggregating by movie')
        final_sims, final_ids, final_metas = aggregate_by_movie(sims, ids, metas, top_k=args.top_k)
        
        # ğŸ†• LLM í•„í„°ë§ (ì„ íƒì )
        if args.llm_filter:
            if not args.openai_key:
                print("[ì˜¤ë¥˜] LLM í•„í„°ë§ì„ ìœ„í•´ì„œëŠ” --openai_keyê°€ í•„ìš”í•©ë‹ˆë‹¤")
            else:
                print('[5/5] LLM Filtering')
                # ë©”íƒ€ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜
                movies_for_filtering = []
                for i, meta in enumerate(final_metas):
                    movies_for_filtering.append({
                        **meta,
                        'score': float(final_sims[0][i])
                    })
                
                # LLM í•„í„°ë§ ì‹¤í–‰
                llm_filter = LLMMovieFilter(args.openai_key)
                filtered_movies = llm_filter.filter_search_results(
                    query=args.demo_query,
                    movies=movies_for_filtering,
                    threshold=args.llm_threshold
                )
                
                # í•„í„°ë§ëœ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ë°°ì—´ í˜•íƒœë¡œ ë³€í™˜
                if filtered_movies:
                    final_sims = np.array([[movie['score'] for movie in filtered_movies]], dtype='float32')
                    final_ids = np.array([[i for i in range(len(filtered_movies))]], dtype='int64')
                    final_metas = filtered_movies
                else:
                    print("[ê²½ê³ ] LLM í•„í„°ë§ í›„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # ê²°ê³¼ ì¶œë ¥
        print_results(final_sims, final_ids, final_metas)
        
        # ğŸ†• ì›¹ ì—°ë™ìš© ê²°ê³¼ ì €ì¥ (ì„ íƒì )
        if args.save_web:
            # ì›¹ í‘œì‹œìš© í¬ë§·ìœ¼ë¡œ ë³€í™˜
            movies_for_web = []
            for i, meta in enumerate(final_metas):
                movies_for_web.append({
                    **meta,
                    'score': float(final_sims[0][i])
                })
            
            web_results = format_results_for_web(
                movies=movies_for_web,
                query=args.demo_query,
                llm_filtered=args.llm_filter
            )
            save_results_for_web(web_results)
            print(f"[ì›¹ ì—°ë™] í¬ìŠ¤í„° ê²½ë¡œ: /static/posters/")
            print(f"[ì›¹ ì—°ë™] íë¦„ê³¡ì„  ë°ì´í„°ë„ ì¶”ê°€ ê°€ëŠ¥")

    data_dir = Path(args.data_dir)
    emb_path = data_dir / 'embeddings.npy'
    meta_path = data_dir / 'metadata.jsonl'
    index_path = data_dir / 'index.faiss'
    idmap_path = data_dir / 'id_map.npy'

    embeddings = np.load(emb_path)          # (N, D)
    metas = load_metadata(meta_path)

    if args.build:
        print('[1/3] Building FAISS index')
        build_faiss_index(embeddings, index_path, idmap_path)
        print(f'âœ… Saved index to {index_path} and id_map to {idmap_path}')

    if args.demo_query:
        print('[2/3] Encoding demo query')
        from sentence_transformers import SentenceTransformer
        import torch
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = SentenceTransformer(args.model, device=device)

        with torch.inference_mode():
            q = model.encode([args.demo_query], convert_to_numpy=True, normalize_embeddings=False).astype('float32')
        q = ensure_unit_norm(q)  # ì¿¼ë¦¬ë„ ì •ê·œí™”

        # ì°¨ì› ê²€ì¦(ëª¨ë¸ í†µì¼ í™•ì¸)
        if q.shape[1] != embeddings.shape[1]:
            raise ValueError(f"Dim mismatch: query({q.shape[1]}) vs corpus({embeddings.shape[1]}). "
                             f"--modelì„ ì„ë² ë”© ìƒì„± ì‹œ ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼í•˜ê²Œ ë§ì¶”ì„¸ìš”.")

        print('[3/3] Searching')
        index = faiss.read_index(str(index_path))
        sims, ids = query_index(index, q, top_k=args.top_k * 3)  # ë” ë§ì´ ê²€ìƒ‰í•´ì„œ ì§‘ê³„

        # ìœˆë„ìš° ì ìˆ˜ë¥¼ ì˜í™”ë³„ë¡œ ì§‘ê³„
        print('[4/4] Aggregating by movie')
        final_sims, final_ids, final_metas = aggregate_by_movie(sims, ids, metas, top_k=args.top_k)
        
        # ğŸ†• LLM í•„í„°ë§ (ì„ íƒì )
        if args.llm_filter:
            if not args.openai_key:
                print("[ì˜¤ë¥˜] LLM í•„í„°ë§ì„ ìœ„í•´ì„œëŠ” --openai_keyê°€ í•„ìš”í•©ë‹ˆë‹¤")
            else:
                print('[5/5] LLM Filtering')
                # ë©”íƒ€ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜
                movies_for_filtering = []
                for i, meta in enumerate(final_metas):
                    movies_for_filtering.append({
                        **meta,
                        'score': float(final_sims[0][i])
                    })
                
                # LLM í•„í„°ë§ ì‹¤í–‰
                llm_filter = LLMMovieFilter(args.openai_key)
                filtered_movies = llm_filter.filter_search_results(
                    query=args.demo_query,
                    movies=movies_for_filtering,
                    threshold=args.llm_threshold
                )
                
                # í•„í„°ë§ëœ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ë°°ì—´ í˜•íƒœë¡œ ë³€í™˜
                if filtered_movies:
                    final_sims = np.array([[movie['score'] for movie in filtered_movies]], dtype='float32')
                    final_ids = np.array([[i for i in range(len(filtered_movies))]], dtype='int64')
                    final_metas = filtered_movies
                else:
                    print("[ê²½ê³ ] LLM í•„í„°ë§ í›„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # ê²°ê³¼ ì¶œë ¥
        print_results(final_sims, final_ids, final_metas)
        
        # ğŸ†• ì›¹ ì—°ë™ìš© ê²°ê³¼ ì €ì¥ (ì„ íƒì )
        if args.save_web:
            # ì›¹ í‘œì‹œìš© í¬ë§·ìœ¼ë¡œ ë³€í™˜
            movies_for_web = []
            for i, meta in enumerate(final_metas):
                movies_for_web.append({
                    **meta,
                    'score': float(final_sims[0][i])
                })
            
            web_results = format_results_for_web(
                movies=movies_for_web,
                query=args.demo_query,
                llm_filtered=args.llm_filter
            )
            save_results_for_web(web_results)
            print(f"[ì›¹ ì—°ë™] í¬ìŠ¤í„° ê²½ë¡œ: /static/posters/")
            print(f"[ì›¹ ì—°ë™] íë¦„ê³¡ì„  ë°ì´í„°ë„ ì¶”ê°€ ê°€ëŠ¥")

def weighted_interactive_search():
    """ê°€ì¤‘ì¹˜ ì¡°ì ˆ ëŒ€í™”í˜• ê²€ìƒ‰"""
    print("=" * 60)
    print("ğŸ­ ê°€ì¤‘ì¹˜ ì¡°ì ˆ ì˜í™” ê²€ìƒ‰ ì‹œìŠ¤í…œ")
    print("=" * 60)
    print("â€¢ ì´ ê¸°ëŠ¥ì€ movie_similarity_finder.pyì—ì„œ ì´ìš©í•˜ì„¸ìš”")
    print("â€¢ í˜„ì¬ëŠ” ê¸°ë³¸ ì²­í‚¹ ê²€ìƒ‰ë§Œ ì§€ì›í•©ë‹ˆë‹¤")
    print("=" * 60)
    
    print("ğŸ’¡ movie_similarity_finder.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ê°€ì¤‘ì¹˜ ì¡°ì ˆ ê²€ìƒ‰ì„ ì´ìš©í•˜ì„¸ìš”!")
    return

if __name__ == '__main__':
    main()