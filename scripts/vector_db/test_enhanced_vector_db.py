"""
í–¥ìƒëœ ì„ë² ë”©ìœ¼ë¡œ ë²¡í„° DB ì¬êµ¬ì¶• ë° í…ŒìŠ¤íŠ¸
"""
import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

def rebuild_vector_db():
    print("ğŸ”§ í–¥ìƒëœ ì„ë² ë”©ìœ¼ë¡œ ë²¡í„° DB ì¬êµ¬ì¶•")
    print("=" * 50)
    
    # í–¥ìƒëœ ì„ë² ë”© ë¡œë“œ
    with open('movie_embeddings_enhanced.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    embeddings = np.array([item['embedding'] for item in data]).astype('float32')
    titles = [item['title'] for item in data]
    
    print(f"ğŸ“Š ì„ë² ë”© ì •ë³´:")
    print(f"   ë²¡í„° ê°œìˆ˜: {len(embeddings)}")
    print(f"   ë²¡í„° ì°¨ì›: {embeddings.shape[1]}")
    
    # FAISS ì¸ë±ìŠ¤ ìƒì„±
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    
    # ì €ì¥
    faiss.write_index(index, 'faiss_movie_index_enhanced.bin')
    
    with open('faiss_movie_meta_enhanced.json', 'w', encoding='utf-8') as f:
        json.dump(titles, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… í–¥ìƒëœ ë²¡í„° DB ì €ì¥ ì™„ë£Œ")
    
    return index, titles

def test_enhanced_search():
    print(f"\nğŸ” í–¥ìƒëœ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("=" * 30)
    
    # ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
    model = SentenceTransformer('all-MiniLM-L6-v2')
    index = faiss.read_index('faiss_movie_index_enhanced.bin')
    
    with open('faiss_movie_meta_enhanced.json', 'r', encoding='utf-8') as f:
        titles = json.load(f)
    
    # ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ í…ŒìŠ¤íŠ¸
    test_queries = [
        "ì•¡ì…˜ ì–´ë“œë²¤ì²˜ ì „ìŸ ì˜í™”",
        "ë¡œë§¨í‹± ì½”ë¯¸ë”” ì‚¬ë‘ ì´ì•¼ê¸°", 
        "ìŠ¤ë¦´ëŸ¬ ì„œìŠ¤íœìŠ¤ ê¸´ì¥ê°",
        "ê³µí¬ í˜¸ëŸ¬ ë¬´ì„œìš´",
        "SF ì‚¬ì´íŒŒì´ ë¯¸ë˜ ê³¼í•™",
        "ë°°ì‹ ë‹¹í•œ ì¡°ì§ì›ë“¤ì—ê²Œ ë³µìˆ˜í•˜ëŠ” ì´ì•¼ê¸°"
    ]
    
    for query in test_queries:
        print(f"\nğŸ¬ ê²€ìƒ‰ì–´: '{query}'")
        
        # ì¿¼ë¦¬ ì„ë² ë”© (ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ)
        query_embedding = model.encode(f"plot: {query}", normalize_embeddings=True)
        
        # ê²€ìƒ‰
        distances, indices = index.search(query_embedding.reshape(1, -1).astype('float32'), k=5)
        
        print(f"   ê²°ê³¼:")
        unique_movies = set()
        result_count = 0
        
        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
            movie_title = titles[idx]
            if movie_title not in unique_movies:  # ì¤‘ë³µ ì œê±°
                unique_movies.add(movie_title)
                result_count += 1
                similarity = 1 / (1 + distance)
                print(f"   {result_count}. {movie_title} (ìœ ì‚¬ë„: {similarity:.3f})")
                
                if result_count >= 3:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                    break
    
    # ë²¡í„° ë‹¤ì–‘ì„± ì¬í™•ì¸
    print(f"\nğŸ“ˆ ë²¡í„° ë‹¤ì–‘ì„± ë¶„ì„:")
    
    # ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ë²¡í„°ë“¤ ê°„ ìœ ì‚¬ë„ í™•ì¸
    sample_indices = np.random.choice(len(titles), min(20, len(titles)), replace=False)
    sample_vectors = np.array([index.reconstruct(i) for i in sample_indices])
    
    # ì •ê·œí™” í›„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    sample_vectors = sample_vectors / np.linalg.norm(sample_vectors, axis=1, keepdims=True)
    similarities = np.dot(sample_vectors, sample_vectors.T)
    
    # ëŒ€ê°ì„  ì œì™¸
    off_diagonal = similarities[np.triu_indices_from(similarities, k=1)]
    
    print(f"   20ê°œ ì˜í™” ìƒ˜í”Œ ê°„ í‰ê·  ìœ ì‚¬ë„: {off_diagonal.mean():.4f}")
    print(f"   ìœ ì‚¬ë„ í‘œì¤€í¸ì°¨: {off_diagonal.std():.4f}")
    print(f"   ìµœëŒ€ ìœ ì‚¬ë„: {off_diagonal.max():.4f}")
    print(f"   ìµœì†Œ ìœ ì‚¬ë„: {off_diagonal.min():.4f}")
    
    if off_diagonal.mean() < 0.5:
        print("   âœ… ë²¡í„° ë‹¤ì–‘ì„± ê°œì„ ë¨")
    else:
        print("   âš ï¸  ì—¬ì „íˆ ê°œì„  í•„ìš”")

if __name__ == "__main__":
    rebuild_vector_db()
    test_enhanced_search()
